
import numpy as np
from numpy import linalg
import math
from sklearn.svm import SVC, NuSVC

#------------------------------------------------------------------------
'''
    L2_SVC_NCH L2-SVCè½¬åŒ–ç‰ˆæœ¬
'''
class L2_SVC_NCH_ByL2SVC(SVC):
    def __init__(self, 
                 kernel_mat, 
                 kernel_name='kernel_gaussian', 
                 gamma=5.0, 
                 C=np.PINF, 
                 myC=1.0, 
                 max_iter=2000, 
                 tol=1e-3, 
                 need_optValue=True):
        super().__init__(kernel=kernel_mat, 
                         gamma=gamma, 
                         C=np.PINF, 
                         max_iter=max_iter, 
                         tol=tol)
        self.myC = myC # Cï¼šç”¨äºæ±‚è§£L2-SVCï¼›myCï¼šç”¨äºL2-SVC-NCHè®¡ç®—pã€q
        self.kernel_name = kernel_name
        kernel_dict = {'kernel_gaussian': self.kernel_gaussian, # æ ¸å‡½æ•°å­—å…¸
                       'kernel_linear': self.kernel_linear,
                       'kernel_quadratic': self.kernel_quadratic}
        self.mykernel = kernel_dict[kernel_name] # æ ¸å‡½æ•°
        self.need_optValue = need_optValue

    def fit(self, X, y):
        self._K = self._gram_matrix(X)
        super().fit(X, y)
        M = np.sum(np.abs(self.dual_coef_.flatten())) / 2.0
        self.alpha_spv = np.abs(self.dual_coef_.flatten()) / M # L2-SVCçš„alpha è½¬åŒ–ä¸º L2-SVC-NCHçš„alpha
        self.y_spv = y[self.support_]
        self.spv = X[self.support_]
        
        mask_positive = self.y_spv > 0
        mask_negative = ~ mask_positive
        alpha_spv_positive = self.alpha_spv[mask_positive] # æ­£ç±»alpha
        alpha_spv_negative = self.alpha_spv[mask_negative] # æ­£ç±»alpha
        ix_spv_positive = self.support_[mask_positive] # æ­£ç±»å…¨å±€ç´¢å¼•
        ix_spv_negative = self.support_[mask_negative] # è´Ÿç±»å…¨å±€ç´¢å¼•

        # è®¡ç®—p (æ­£ä¾‹)ã€q (è´Ÿä¾‹)
        self.p = self.get_pq(y, alpha_spv_positive, ix_spv_positive)
        self.q = self.get_pq(y, alpha_spv_negative, ix_spv_negative)

        # è®¡ç®—ç›®æ ‡å‡½æ•°å€¼
        if self.need_optValue:
            self.opt_value = self.get_optValue(self.support_, self.y_spv, self.alpha_spv)
        # self.optValue = 0.5 * np.dot(self.alpha_spv.T, self._K[self.support_][self.support_], self.alpha_spv)
        # print('æ­£ä¾‹alphaä¹‹å’Œä¸ºï¼š{0}ï¼Œè´Ÿä¾‹alphaä¹‹å’Œä¸º{1}'.format(np.sum(alpha_spv_positive), np.sum(alpha_spv_negative)))

    # æ ¸çŸ©é˜µè®¡ç®—
    def _gram_matrix(self, X):
        # é«˜æ–¯æ ¸ä¼˜åŒ–
        if self.kernel_name == 'kernel_gaussian':
            return np.exp(-self.gamma * ((X**2).sum(1).reshape(-1, 1) + (X**2).sum(1) - 2 * X @ X.T))
        else:
            m, _ = X.shape
            K = np.zeros((m, m), dtype=np.float64)
            for i in range(m):
                for j in range(i, m):
                    K[i][j] = self.mykernel(X[i], X[j])
                    K[j][i] = K[i][j]
            return K

    # è®¡ç®—pï¼ˆæ­£ç±»ï¼‰,qï¼ˆè´Ÿç±»ï¼‰
    def get_pq(self, y, alpha, ix_list):
        p = 0.0
        for i, ix in enumerate(ix_list):
            p += y[ix] * alpha[i] * (1.0 / self.myC)
            p += np.sum(alpha * y[ix_list] * self._K[ix, ix_list])
        # å–å¹³å‡
        return p / len(ix_list)


    # è¿”å›ç›®æ ‡å‡½æ•°å€¼
    def get_optValue(self, ix_list, y, alpha):
        opt_value = 0.0
        for i, ix_i in enumerate(ix_list):
            for j, ix_j in enumerate(ix_list):
                opt_value += alpha[i] * alpha[j] * y[i] * y[j] * self._K[ix_i][ix_j]
        opt_value += (1.0 / self.C) * np.sum(alpha * alpha)
        opt_value /= 2
        return opt_value
        
    # é¢„æµ‹å‡½æ•°
    def project(self, X):
        ''''''
        y_pred = np.zeros(len(X))
        for i in range(len(X)):
            s = 0.0
            for _spv, _y_spv, _alpha_spv in zip(self.spv, self.y_spv, self.alpha_spv):
                s += _y_spv * _alpha_spv * self.mykernel(X[i], _spv)
            y_pred[i] = s
        y_pred = y_pred - (self.p + self.q) / 2
        return y_pred

    # é¢„æµ‹
    def predict(self, X):
        ''''''
        # print('å¼€å§‹é¢„æµ‹ï¼š')
        return np.sign(self.project(X))

    # é«˜æ–¯æ ¸å‡½æ•°
    def kernel_gaussian(self, x1, x2):
        return np.exp(-self.gamma * (linalg.norm(x1 - x2) ** 2))
        # return np.exp(-linalg.norm(x1-x2) ** 2 / (2 * (self.gamma ** 2)))
    def kernel_linear(self, x1, x2):
        return np.dot(x1, x2.T)
    def kernel_quadratic(self, x1, x2):
        return (np.dot(x1, x2.T) ** 2)
    
# --------------------------------------------------------------------
'''
    L2_SVC_NCH åŸç”Ÿpythonç‰ˆæœ¬
'''
class L2_SVC_NCH_Python():
    def __init__(self, 
                 kernel_name='kernel_gaussian', 
                 C=1.0, 
                 gamma=5.0, 
                 max_iter=2000, 
                 epsilon=1e-3, 
                 need_optValue=True):
        self.C = C # æƒ©ç½šç³»æ•°
        self.gamma = gamma # é«˜æ–¯å‚æ•°
        self.max_iter = max_iter # æœ€å¤§è¿­ä»£æ¬¡æ•°
        self.epsilon = epsilon # è¯¯å·®ç²¾åº¦
        self.kernel_name = kernel_name
        kernel_dict = {'kernel_gaussian': self.kernel_gaussian, # æ ¸å‡½æ•°å­—å…¸
                       'kernel_linear': self.kernel_linear,
                       'kernel_quadratic': self.kernel_quadratic}
        self.kernel = kernel_dict[kernel_name] # æ ¸å‡½æ•°
        self.need_optValue = need_optValue

    def fit(self, X, y):
        ''''''
        self.m, _ = X.shape # mæ ·æœ¬æ•°ï¼Œnç‰¹å¾æ•°
        self.K = self._gram_matrix(X) # è®¡ç®—Kij
        self.G = np.outer(y.T, y) * self.K # Gij = yi*yj*Kij
        self.count = 0 # è®¡æ•°å™¨ è®°å½•è¿­ä»£æ¬¡æ•°

        # ç”Ÿæˆç”¨äºåŒºåˆ†æ­£ç±»å’Œè´Ÿç±»çš„mask
        mask_positive = (y == 1) 
        mask_negative = ~ mask_positive 
        # æ­£ç±»å’Œè´Ÿç±»çš„å…¨å±€ç´¢å¼•åˆ—è¡¨
        ix = np.arange(len(y))
        ix_positive = np.arange(len(y))[mask_positive]
        ix_negative = np.arange(len(y))[mask_negative]
        # alphaåˆå§‹åŒ–
        # alpha = self.alpha_init(alpha, ix_positive, ix_negative)
        alpha = np.ones(len(y), dtype=np.float64)
        alpha[ix_positive] *= 1.0 / len(ix_positive)
        alpha[ix_negative] *= 1.0 / len(ix_negative)
        # kkt and picked
        iskkt, is_picked_all = False, False
        # print('å¼€å§‹æ±‚è§£alpha by SMO...')
        while True:
            # è®¡æ•°
            self.count += 1
            # print('è¿­ä»£æ¬¡æ•°ï¼š', count)
            # alpha_pre = np.copy(alpha) # æ·±æ‹·è´ï¼ˆ'='æ˜¯åœ°å€å¼•ç”¨ï¼‰ç”¨äºåé¢çš„æ¯”è¾ƒåˆ¤åˆ«åœæœºæ¡ä»¶
            
            if iskkt == False and is_picked_all == False:
                # Maximal Violoting Pair
                i, j, class_name, iskkt, is_picked_all = self.get_ij_byMVP(alpha, ix_positive, ix_negative)
                alpha_old_i, alpha_old_j = alpha[i], alpha[j]
                # print('iskktï¼š                     ', iskkt)
                # print('class_nameï¼š                ', class_name)
                # print('jï¼Œ           iï¼š           ', j, i)
                # print('alpha[j],     alpha[i]ï¼š    ', alpha[j], alpha[i])
            if iskkt:
                print('å·²æ»¡è¶³kktï¼')
                break
            if is_picked_all:
                print('å·²æ— è¿åå¯¹ï¼')
                break
            if class_name == 'positive': # å¤„ç†æ­£ç±»
                alpha[j] = self.get_alpha_j(i, j, alpha, y) # è®¡ç®—alpha_j unclip
                # clip
                if alpha[j] > alpha_old_i + alpha_old_j:
                    alpha[j] = alpha_old_i + alpha_old_j
                    alpha[i] = 0
                elif alpha[j] < 0:
                    alpha[j] = 0
                    alpha[i] = alpha_old_i + alpha_old_j
                else:
                    # alpha[j] = alpha[j]
                    alpha[i] = alpha_old_i + alpha_old_j - alpha[j]
                # alpha[j] = self.cilp(alpha[j]) # clip
                # if math.isclose(alpha[j], 1): # å¦‚æœalpha_jä¸º1ï¼Œåˆ™å…¶ä»–alphaç½®0
                #     alpha = self.clear_alpha_except_j(j, alpha, ix_positive)
                # alpha[i] = self.get_alpha_i(i, j, alpha, alpha[j], ix_positive) # è®¡ç®—alpha_i 
            
            elif class_name == 'negative': # å¤„ç†è´Ÿç±»
                alpha[j] = self.get_alpha_j(i, j, alpha, y) # unclip
                # clip
                if alpha[j] > alpha_old_i + alpha_old_j:
                    alpha[j] = alpha_old_i + alpha_old_j
                    alpha[i] = 0
                elif alpha[j] < 0:
                    alpha[j] = 0
                    alpha[i] = alpha_old_i + alpha_old_j
                else:
                    # alpha[j] = alpha[j]
                    alpha[i] = alpha_old_i + alpha_old_j - alpha[j]
                # alpha[j] = self.cilp(alpha[j])
                # if math.isclose(alpha[j], 1):
                #     alpha = self.clear_alpha_except_j(j, alpha, ix_negative)
                # alpha[i] = self.get_alpha_i(i, j, alpha, alpha[j], ix_negative)
            else:
                print('error!')
            # print('alpha[j]_new, alpha[i]_newï¼š', alpha[j], alpha[i])
            # print('ğŸ”ºalpha[j]ï¼Œ ğŸ”ºalpha[i]ï¼š  ', alpha[j] - alpha_pre[j], alpha[i] - alpha_pre[i])     
            # print('alpha[ix_positive]ï¼š', alpha[ix_positive])
            # print('alpha[ix_negative]ï¼š', alpha[ix_negative])
           
            # è®¡ç®—difference
            # diff = np.linalg.norm(alpha - alpha_pre)
            # # print('alpha diffï¼š', diff)
            # if diff < 1e-7:
            #     break

            # æœ€å¤§è¿­ä»£æ¬¡æ•°
            if self.count >= self.max_iter:
                print('å·²è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°{0}ï¼'.format(self.max_iter)) 
                break
            
        # print('alphaæ±‚è§£ç»“æŸ')

        # æ”¯æŒå‘é‡å¸ƒå°”åˆ—è¡¨
        self.mask_spv = alpha > 1e-6 
        # è·å–æ”¯æŒå‘é‡ç´¢å¼•ã€æ”¯æŒå‘é‡ã€yå’Œalpha
        self.ix_spv = np.arange(len(y))[self.mask_spv]
        self.spv = X[self.mask_spv]
        self.y_spv = y[self.mask_spv]
        self.alpha_spv = alpha[self.mask_spv]

        # è·å–æ­£ä¾‹å’Œè´Ÿä¾‹æ”¯æŒå‘é‡ç´¢å¼•
        ix_spv_positive = np.arange(len(y))[self.mask_spv & mask_positive]
        ix_spv_negative = np.arange(len(y))[self.mask_spv & mask_negative]

        # è®¡ç®—p (æ­£ä¾‹)ã€q (è´Ÿä¾‹)
        self.p = self.get_pq(y, alpha, ix_spv_positive)
        self.q = self.get_pq(y, alpha, ix_spv_negative)

        if self.need_optValue:
            self.opt_value = self.get_optValue(self.ix_spv, self.y_spv, self.alpha_spv)
        # print('opt_value: ', self.opt_value)
        # print('æ­£ä¾‹alphaä¹‹å’Œä¸ºï¼š{0}ï¼Œè´Ÿä¾‹alphaä¹‹å’Œä¸º{1}'.format(np.sum(alpha[ix_spv_positive]), np.sum(alpha[ix_spv_negative])))

    # è®¡ç®—pï¼ˆæ­£ç±»ï¼‰,qï¼ˆè´Ÿç±»ï¼‰
    def get_pq(self, y, alpha, ix):
        p = 0.0
        for i in ix:
            p += y[i] * alpha[i] / self.C
            p += np.sum(alpha[ix] * y[ix] * self.K[i, ix])
        # å–å¹³å‡
        return p / len(ix)

    # Maximal Violoting Pairï¼ˆæœ€å¤§è¿åå¯¹åŸåˆ™ï¼‰é€‰å–alpha_i, alpha_j
    def get_ij_byMVP(self, alpha, ix_positive, ix_negative):
        # åˆå§‹åŒ–
        i, j = 0, 0
        ix_m_positive, ix_M_positive, ix_m_negative, ix_M_negative = 0, 0, 0, 0
        m_positive, M_positive, m_negative, M_negative = 0.0, 0.0, 0.0, 0.0
        class_name = '' # æœ¬æ¬¡è¿åå¯¹é€‰å–çš„ç±»æ¯”ï¼šæ­£ç±»orè´Ÿç±»
        is_kkt = False
        is_pick_all_positive, is_pick_all_negative = True, True # é˜²æ­¢å‡ºç°ç©ºå€™é€‰é›†

        # å¾…é€‰å–åˆ—è¡¨
        _list = - np.dot((self.G + (1.0 / self.C) * np.identity(self.m)), alpha)
        
        # å®šä¹‰å€™é€‰é›†ï¼šç´¢å¼•é›†åˆ
        ix_positive_up = ix_positive[alpha[ix_positive] < 1]
        ix_positive_down = ix_positive[alpha[ix_positive] > 1e-6]
        ix_negative_up = ix_negative[alpha[ix_negative] > 1e-6]
        ix_negative_down = ix_negative[alpha[ix_negative] < 1]
        
        # æ±‚æœ€å¤§è¿åå¯¹åŠå¯¹åº”çš„ç´¢å¼•
        if ix_positive_up.size != 0 and ix_positive_down.size != 0:
            _ix_m_positive = np.argmax(_list[ix_positive_up])
            ix_m_positive = ix_positive_up[_ix_m_positive]
            m_positive = _list[ix_m_positive]
            _ix_M_positive = np.argmin(_list[ix_positive_down])
            ix_M_positive = ix_positive_down[_ix_M_positive]
            M_positive = _list[ix_M_positive]
            is_pick_all_positive = False
        
        if ix_negative_up.size != 0 and ix_negative_down.size != 0:
            _ix_m_negative = np.argmax(_list[ix_negative_down])
            ix_m_negative = ix_negative_down[_ix_m_negative]
            m_negative = _list[ix_m_negative]
            _ix_M_negative = np.argmin(_list[ix_negative_up])
            ix_M_negative = ix_negative_up[_ix_M_negative]
            M_negative = _list[ix_M_negative]
            is_pick_all_negative = False

        # if self.count == self.max_iter-1:
        #     # print('iter: {0}, opt_value: {1}'.format(self.count, self.opt_value))
        #     # print('iter, m_+, M_+, m_-, M_-', self.count, m_positive, M_positive, m_negative, M_negative)
        #     print('iter: {0}, (m_+ - M_+): {1}, (m_- - M_-): {2}'.format(self.count, (m_positive - M_positive),(m_negative - M_negative)))
        
        # æ»¡è¶³KKTæ¡ä»¶
        if m_positive <= M_positive + self.epsilon and m_negative <= M_negative + self.epsilon:
            is_kkt = True
        # å¯¹æ¯”æ­£ã€è´Ÿè¿åå¯¹ï¼Œé€‰å–è¿æ³•ç¨‹åº¦æ›´å¤§çš„é‚£ç±»ä½œä¸ºè¿”å›
        else:
            if is_pick_all_positive and is_pick_all_negative:
                class_name = 'none'
            elif is_pick_all_positive or is_pick_all_negative:
                class_name = 'positive' if is_pick_all_positive == False else 'negative'
            else:
                class_name = 'positive' if (m_positive - M_positive) > (m_negative - M_negative) else 'negative'
            i, j = (ix_m_positive, ix_M_positive) if class_name == 'positive' else (ix_m_negative, ix_M_negative)

        return i, j, class_name, is_kkt, (is_pick_all_positive and is_pick_all_negative)

    # clear_alpha_except_j
    # def clear_alpha_except_j(self, j, alpha, ix_list):
    #     w = np.ones(len(alpha), dtype=np.float64)
    #     w[ix_list] = 0.0
    #     new_alpha = w * alpha
    #     new_alpha[j] = 1.0
    #     return new_alpha

    # è®¡ç®—alpha_j
    def get_alpha_j(self, i, j, alpha, y):
        ''''''        
        # wç›¸å½“äºæ©ç ä½œç”¨ï¼Œå°†iå’Œjå¤„ç½®0ï¼ˆä¸å‚ä¸è®¡ç®—ï¼‰
        w = np.ones(len(y))
        w[i], w[j] = 0, 0
        formula_up = (alpha[i] + alpha[j]) * self.K[i][i] \
            - (alpha[i] + alpha[j]) * y[i] * y[j] * self.K[i][j] \
            + y[i] * np.sum(w * alpha * y * self.K[i, :]) \
            - y[j] * np.sum(w * alpha * y * self.K[j, :]) \
            + (1.0 / self.C) * (alpha[i] + alpha[j])
        formula_down = self.K[i][i] + self.K[j][j] - 2 * y[i] * y[j] * self.K[i][j] \
            + 2 * (1.0 / self.C)
        alpha_j = formula_up / formula_down
        return alpha_j

    # è£å‰ª
    # def cilp(self, alpha_j):
    #     ''''''
    #     if alpha_j < 1e-6:
    #         alpha_j = 0
    #     elif alpha_j > 1:
    #         alpha_j = 1
    #     else:
    #         pass
    #     return alpha_j

    # è·å–alpha_i    
    # def get_alpha_i(self, i, j, alpha, alpha_j, ix_list):
    #     # å»é™¤i, j
    #     w = np.ones(len(alpha))
    #     w[i], w[j] = 0, 0
    #     w = w[ix_list]
    #     alpha_i = 1 - np.sum(w * alpha[ix_list]) - alpha_j
    #     return alpha_i
    
    # æ ¸çŸ©é˜µè®¡ç®—
    def _gram_matrix(self, X):
        if self.kernel_name == 'kernel_gaussian':
            return np.exp(-self.gamma * ((X**2).sum(1).reshape(-1, 1) + (X**2).sum(1) - 2 * X @ X.T))
        else:
            m, _ = X.shape
            K = np.zeros((m, m))
            for i in range(m):
                for j in range(i, m):
                    K[i][j] = self.kernel(X[i], X[j])
                    K[j][i] = K[i][j]
            return K

    # è¿”å›ç›®æ ‡å‡½æ•°å€¼
    def get_optValue(self, ix_list, y, alpha):
        opt_value = 0.0
        for i, ix_i in enumerate(ix_list):
            for j, ix_j in enumerate(ix_list):
                opt_value += alpha[i] * alpha[j] * y[i] * y[j] * self.K[ix_i][ix_j]
        opt_value += (1.0 / self.C) * np.sum(alpha * alpha)
        opt_value /= 2
        return opt_value

    # é¢„æµ‹å‡½æ•°
    def project(self, X):
        ''''''
        y_pred = np.zeros(len(X))
        for i in range(len(X)):
            s = 0.0
            for _spv, _y_spv, _alpha_spv in zip(self.spv, self.y_spv, self.alpha_spv):
                s += _y_spv * _alpha_spv * self.kernel(X[i], _spv)
            y_pred[i] = s
        y_pred = y_pred - (self.p + self.q) / 2
        return y_pred

    # é¢„æµ‹
    def predict(self, X):
        ''''''
        # print('å¼€å§‹é¢„æµ‹ï¼š')
        return np.sign(self.project(X))

    # é«˜æ–¯æ ¸å‡½æ•°
    def kernel_gaussian(self, x1, x2):
        return np.exp(-self.gamma * linalg.norm(x1 - x2) ** 2)
        # return np.exp(-linalg.norm(x1-x2) ** 2 / (2 * (self.gamma ** 2)))
    def kernel_linear(self, x1, x2):
        return np.dot(x1, x2.T)
    def kernel_quadratic(self, x1, x2):
        return (np.dot(x1, x2.T) ** 2)

# --------------------------------------------------------------------
'''
    L2_SVC_NCH Nu-SVCè½¬åŒ–ç‰ˆ
        - Lagrangeç³»æ•°å­˜åœ¨åå·®ï¼Œæš‚ä¸å¯ç”¨
'''
# class L2_SVC_NCH_ByNuSVC(NuSVC):
#     def __init__(self, kernel_mat, kernel_name='kernel_gaussian', gamma=5.0, C=1.0, nu=0.5, max_iter=2000, tol=1e-3):
#         super().__init__(kernel=kernel_mat, nu=nu, gamma=gamma, max_iter=max_iter, tol=tol)
#         self.C = C # æƒ©ç½šç³»æ•°
#         kernel_dict = {'kernel_gaussian': self.kernel_gaussian, # æ ¸å‡½æ•°å­—å…¸
#                        'kernel_linear': self.kernel_linear,
#                        'kernel_quadratic': self.kernel_quadratic}
#         self.mykernel = kernel_dict[kernel_name] # æ ¸å‡½æ•°

#     def fit(self, X, y):
#         ''''''
#         self._K = self._gram_matrix(X)
#         super().fit(X, y)

#         self.alpha_spv = np.abs(self.dual_coef_.flatten())
#         self.y_spv = y[self.support_]
#         self.spv = X[self.support_]
        
#         mask_positive = self.y_spv > 0
#         mask_negative = ~ mask_positive
#         alpha_spv_positive = self.alpha_spv[mask_positive] # æ­£ç±»alpha
#         alpha_spv_negative = self.alpha_spv[mask_negative] # æ­£ç±»alpha
#         ix_spv_positive = self.support_[mask_positive] # æ­£ç±»å…¨å±€ç´¢å¼•
#         ix_spv_negative = self.support_[mask_negative] # è´Ÿç±»å…¨å±€ç´¢å¼•

#         # è®¡ç®—p (æ­£ä¾‹)ã€q (è´Ÿä¾‹)
#         self.p = self.get_pq(y, alpha_spv_positive, ix_spv_positive)
#         self.q = self.get_pq(y, alpha_spv_negative, ix_spv_negative)
        
#         print('æ­£ä¾‹alphaä¹‹å’Œä¸ºï¼š{0}ï¼Œè´Ÿä¾‹alphaä¹‹å’Œä¸º{1}'.format(np.sum(alpha_spv_positive), np.sum(alpha_spv_negative)))


#     # æ ¸çŸ©é˜µè®¡ç®—
#     def _gram_matrix(self, X):
#         m, _ = X.shape
#         K = np.zeros((m, m), dtype=np.float64)
#         for i in range(m):
#             for j in range(i, m):
#                 K[i][j] = self.mykernel(X[i], X[j])
#                 K[j][i] = K[i][j]
#         return K

#     # è®¡ç®—pï¼ˆæ­£ç±»ï¼‰,qï¼ˆè´Ÿç±»ï¼‰
#     def get_pq(self, y, alpha, ix_list):
#         p = 0.0
#         for i, ix in enumerate(ix_list):
#             p += y[ix] * alpha[i] * (1.0 / self.C)
#             p += np.sum(alpha * y[ix_list] * self._K[ix, ix_list])
#         # å–å¹³å‡
#         return p / len(ix_list)

#     # é¢„æµ‹å‡½æ•°
#     def project(self, X):
#         ''''''
#         y_pred = np.zeros(len(X))
#         for i in range(len(X)):
#             s = 0.0
#             for _spv, _y_spv, _alpha_spv in zip(self.spv, self.y_spv, self.alpha_spv):
#                 s += _y_spv * _alpha_spv * self.mykernel(X[i], _spv)
#             y_pred[i] = s
#         y_pred = y_pred - (self.p + self.q) / 2
#         return y_pred

#     # é¢„æµ‹
#     def predict(self, X):
#         ''''''
#         # print('å¼€å§‹é¢„æµ‹ï¼š')
#         return np.sign(self.project(X))

#     # é«˜æ–¯æ ¸å‡½æ•°
#     def kernel_gaussian(self, x1, x2):
#         return np.exp(-self.gamma * (linalg.norm(x1 - x2) ** 2))
#         # return np.exp(-linalg.norm(x1-x2) ** 2 / (2 * (self.gamma ** 2)))

#     def kernel_linear(self, x1, x2):
#         return np.dot(x1, x2.T)
#     def kernel_quadratic(self, x1, x2):
#         return (np.dot(x1, x2.T) ** 2)
    
